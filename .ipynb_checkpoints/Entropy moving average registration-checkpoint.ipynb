{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e65b8d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 18:01:24.797528: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/shantanu.singh/miniconda3/envs/py37/lib\n",
      "2022-02-25 18:01:24.797568: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import quaternion\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a317cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /scratch/shantanu/results_23022022/occant_rgb/dump/0/Cantwell/front/bev_pred\n",
    "# !ls /scratch/shantanu/gibson4/new/Cantwell/0/front/pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d7ebb896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta : momentum for moving average update\n",
    "beta = 0.99##\n",
    "threshold_explored = 0.5 ##\n",
    "threshold_entropy = 0.5 ##\n",
    "EPS_MAPPER = 1e-6\n",
    "s = 1\n",
    "LOCAL_MAP_SIZE = 128##\n",
    "OUTMAP_SIZE = 1024##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d733c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(m,p_reg):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        m - ( 2, M, M) - global map\n",
    "        p_reg - ( 2, M, M) - registered egomap\n",
    "    \"\"\"\n",
    "#     explored_mask = (p_reg[:, 1] > threshold_explored).float()\n",
    "    explored_mask = (p_reg[1] > threshold_explored)\n",
    "    '''\n",
    "    Looks like they want sone kind of count on \n",
    "    number of observations for that cell\n",
    "    '''\n",
    "    \n",
    "    log_p_reg = np.log(p_reg + EPS_MAPPER)\n",
    "    log_1_p_reg = np.log(1 - p_reg + EPS_MAPPER)\n",
    "#     log_p_reg = torch.log(p_reg + EPS_MAPPER)\n",
    "#     log_1_p_reg = torch.log(1 - p_reg + EPS_MAPPER)\n",
    "    entropy = -p_reg * log_p_reg - (1 - p_reg) * log_1_p_reg\n",
    "#     print(\"Entropy shape : \",entropy.shape)\n",
    "#     entropy_mask = (entropy.mean(dim=1) < threshold_entropy).float()\n",
    "    '''\n",
    "    Entroypy mask mean along ???\n",
    "    '''\n",
    "    entropy_mask = (entropy.mean(axis=0) < threshold_entropy)\n",
    "    \n",
    "    explored_mask = explored_mask * entropy_mask\n",
    "#     unfilled_mask = (m[:, 1] == 0).float()\n",
    "    unfilled_mask = (m[1] == 0)\n",
    "    \n",
    "    m_updated = m\n",
    "    # For regions that are unfilled, write as it is\n",
    "    mask = unfilled_mask * explored_mask\n",
    "#     mask = mask.unsqueeze(1)\n",
    "    m_updated = m_updated * (1 - mask) + p_reg * mask\n",
    "    # For regions that are filled, do a moving average\n",
    "    mask = (1 - unfilled_mask) * explored_mask\n",
    "#     mask = mask.unsqueeze(1)\n",
    "    p_reg_ma = (p_reg * (1 - beta) + m_updated * beta) * mask\n",
    "    m_updated = m_updated * (1 - mask) + p_reg_ma * mask\n",
    "    \n",
    "    return m_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4988c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_transform_map(p, x, invert=True, mode=\"linear\"):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        p     - ( f, H, W) Tensor\n",
    "        x     - (3) Tensor (x, y, theta) transforms to perform\n",
    "    Outputs:\n",
    "        p_trans - (bs, f, H, W) Tensor\n",
    "    Conventions:\n",
    "        Shift in X is rightward, and shift in Y is downward. Rotation is clockwise.\n",
    "    Note: These denote transforms in an agent's position. Not the image directly.\n",
    "    For example, if an agent is moving upward, then the map will be moving downward.\n",
    "    To disable this behavior, set invert=False.\n",
    "    \"\"\"\n",
    "#     device = p.device\n",
    "#     H, W = p.shape[2:]\n",
    "    H, W = p.shape[1:]\n",
    "    \n",
    "#     trans_x = x[:, 0]\n",
    "#     trans_y = x[:, 1]\n",
    "    trans_x = x[0]\n",
    "    trans_y = x[1]\n",
    "    # Convert translations to -1.0 to 1.0 range\n",
    "    Hby2 = (H - 1) / 2 if H % 2 == 1 else H / 2\n",
    "    Wby2 = (W - 1) / 2 if W % 2 == 1 else W / 2\n",
    "\n",
    "    trans_x = trans_x / Wby2\n",
    "    trans_y = trans_y / Hby2\n",
    "#     rot_t = x[:, 2]\n",
    "    rot_t = x[2]\n",
    "\n",
    "#     sin_t = torch.sin(rot_t)\n",
    "#     cos_t = torch.cos(rot_t)\n",
    "    sin_t = np.sin(rot_t)\n",
    "    cos_t = np.cos(rot_t)\n",
    "    # This R convention means Y axis is downwards.\n",
    "#     A = np.zeros((p.size(0), 3, 3)).to(device)\n",
    "#     A = np.zeros((p.shape[0], 3, 3))\n",
    "    A = np.zeros((3, 3))\n",
    "    \n",
    "#     A[:, 0, 0] = cos_t\n",
    "#     A[:, 0, 1] = -sin_t\n",
    "#     A[:, 1, 0] = sin_t\n",
    "#     A[:, 1, 1] = cos_t\n",
    "#     A[:, 0, 2] = trans_x\n",
    "#     A[:, 1, 2] = trans_y\n",
    "#     A[:, 2, 2] = 1\n",
    "    A[0, 0] = cos_t\n",
    "    A[0, 1] = -sin_t\n",
    "    A[1, 0] = sin_t\n",
    "    A[1, 1] = cos_t\n",
    "    A[0, 2] = trans_x\n",
    "    A[1, 2] = trans_y\n",
    "    A[2, 2] = 1\n",
    "    # Since this is a source to target mapping, and F.affine_grid expects\n",
    "    # target to source mapping, we have to invert this for normal behavior.\n",
    "#     Ainv = torch.inverse(A)\n",
    "    Ainv = np.linalg.inv(A)\n",
    "    \n",
    "\n",
    "    # If target to source mapping is required, invert is enabled and we invert\n",
    "    # it again.\n",
    "    if invert:\n",
    "#         Ainv = torch.inverse(Ainv)\n",
    "        Ainv = np.linalg.inv(Ainv)\n",
    "        \n",
    "#     Ainv = Ainv[:, :2]\n",
    "#     Ainv = Ainv[:2]\n",
    "#     print(Ainv)\n",
    "#     print(p.shape,Ainv.shape)\n",
    "   \n",
    "    p_trans = scipy.ndimage.affine_transform(p,Ainv)\n",
    "    print(np.max(p_trans))\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(p[0])\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(p_trans[0])\n",
    "#     grid = F.affine_grid(Ainv, p.size())\n",
    "#     print(np.unique(p))\n",
    "#     plt.\n",
    "#     p_trans = F.grid_sample(p, grid, mode=mode)\n",
    "    '''\n",
    "    Initially it was mode=\"bilinear \" but in scipy.interpolate.griddata ,\n",
    "    no such mode is present. So i set the default to \"linear\"\n",
    "    '''\n",
    "#     p_trans = scipy.interpolate.griddata(p, grid, mode=mode)\n",
    "\n",
    "    return p_trans\n",
    "\n",
    "def spatial_transform(p, dx, invert=False):\n",
    "    # Convert dx to map image coordinate system with X as rightward and Y as downward\n",
    "#     dx_map = torch.stack(\n",
    "#         [(dx[:, 1] / s), -(dx[:, 0] / s), dx[:, 2]], dim=1\n",
    "#     )  # anti-clockwise rotation\n",
    "#     dx_map = np.stack(\n",
    "#         [(dx[:, 1] / s), -(dx[:, 0] / s), dx[:, 2]], axis=1\n",
    "#     )\n",
    "    dx_map = np.stack(\n",
    "        [(dx[1] / s), -(dx[0] / s), dx[2]]\n",
    "    )\n",
    "#     print(dx_map)\n",
    "    p_trans = spatial_transform_map(p, dx_map, invert=invert)\n",
    "\n",
    "    return p_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0065dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_map(m,p,x):\n",
    "    \"\"\"\n",
    "    Given the locally computed map, register it to the global map based\n",
    "    on the current position.\n",
    "    Inputs:\n",
    "    m - ( F, M, M) global map\n",
    "    p - ( F, V, V) local map\n",
    "    x - ( 3) in global coordinates\n",
    "    \"\"\"\n",
    "    V = LOCAL_MAP_SIZE\n",
    "    M = m.shape[2]\n",
    "    Vby2 = (V - 1) // 2 if V % 2 == 1 else V // 2\n",
    "    Mby2 = (M - 1) // 2 if M % 2 == 1 else M // 2\n",
    "    # The agent stands at the bottom-center of the egomap and looks upward\n",
    "    left_h_pad = Mby2 - V + 1\n",
    "    right_h_pad = M - V - left_h_pad\n",
    "    left_w_pad = Mby2 - Vby2\n",
    "    right_w_pad = M - V - left_w_pad\n",
    "    # Add zero padding to p so that it matches size of global map\n",
    "#     print(p.shape)\n",
    "    p_pad = np.zeros((p.shape[0],M,M))\n",
    "    p_pad[:,left_h_pad:left_h_pad+V,left_w_pad:left_w_pad+V] = p\n",
    "#     print(p_pad.shape)\n",
    "\n",
    "#     p_pad = F.pad(\n",
    "#             p, (left_w_pad, right_w_pad, left_h_pad, right_h_pad), \"constant\", value=0\n",
    "#         )\n",
    "#     p_pad = np.pad(\n",
    "#     p, np.array([(left_w_pad, right_w_pad, left_h_pad, right_h_pad),(left_w_pad, right_w_pad, left_h_pad, right_h_pad)]).T, \"constant\")\n",
    "    # Register the local map\n",
    "    p_reg = spatial_transform(p_pad, x)\n",
    "    # Aggregate\n",
    "    m_updated = aggregate(m, p_reg)\n",
    "\n",
    "    return m_updated\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1837bf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.973924884761472e-165\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "LOCAL_MAP_DIR  = \"/scratch/shantanu/results_23022022/occant_rgb/dump/0/Cantwell/front/bev_pred\"\n",
    "POSE_DIR  = \"/scratch/shantanu/gibson4/new/Cantwell/0/front/pose\"\n",
    "\n",
    "def register_all_observations():\n",
    "#     m = np.zeros((1,2,OUTMAP_SIZE,OUTMAP_SIZE))\n",
    "    m = np.zeros((2,OUTMAP_SIZE,OUTMAP_SIZE))\n",
    "#     m = tf.convert_to_tensor(m)\n",
    "    local_maps = os.listdir(LOCAL_MAP_DIR)\n",
    "    for local_map in local_maps[3:4]:\n",
    "        scan = local_map.split('.')[0]\n",
    "        pose = np.load(POSE_DIR+'/'+scan+'.npy',allow_pickle=True).item()\n",
    "        px,pz = pose['position'][0],pose['position'][2]\n",
    "        quat = pose['rotation']\n",
    "        yaw = quaternion.as_euler_angles(quat)[2]\n",
    "#         x = np.array([[px,pz,yaw]])\n",
    "        x = np.array([px,pz,yaw])\n",
    "        \n",
    "#         x = tf.convert_to_tensor(x)\n",
    "#         print(x)\n",
    "        p = cv2.imread(LOCAL_MAP_DIR+'/'+local_map,-1)\n",
    "        unknown_p = (1-(p==0))\n",
    "        p_reg = np.zeros((2,LOCAL_MAP_SIZE,LOCAL_MAP_SIZE))\n",
    "        p_reg[0,:] = p\n",
    "        p_reg[1,:] = unknown_p\n",
    "#         plt.imshow(unknown_p)\n",
    "#         print(p_reg.shape)\n",
    "#         print(np.unique(p_reg))\n",
    "#         p_reg = np.array([np.moveaxis(np.array(p_reg),-1,0)])\n",
    "#         p_reg = np.moveaxis(np.array(p_reg),-1,0)\n",
    "#         p_reg = tf.convert_to_tensor(p_reg)\n",
    "#         print(p_reg)\n",
    "        m = register_map(m,p_reg,x)\n",
    "    return m\n",
    "m = register_all_observations()\n",
    "# plt.imshow(m[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
